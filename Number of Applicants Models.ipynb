{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data if necessary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Model_Test_Data.xlsx'\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e1259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Regression model of Real Total Entries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Filter data for the years 2010-2023\n",
    "df_filtered = df[(df['Year'] >= 2010) & (df['Year'] <= 2023)]\n",
    "\n",
    "# Group by unique combinations and sum 'Real_Total_Entries'\n",
    "grouped_data = df_filtered.groupby(['Item_Description', 'District', 'Year'])['Real_Total_Entries'].sum().reset_index()\n",
    "\n",
    "# Linear Regression for each unique combination\n",
    "for index, combination in grouped_data.iterrows():\n",
    "    plt.figure(figsize=(12, 8))  # Create a new figure for each unique combination\n",
    "\n",
    "    subset = grouped_data[\n",
    "        (grouped_data['Item_Description'] == combination['Item_Description']) &\n",
    "        (grouped_data['District'] == combination['District'])\n",
    "    ]\n",
    "\n",
    "    # Prepare the data for linear regression\n",
    "    X = subset['Year'].values.reshape(-1, 1)\n",
    "    y = subset['Real_Total_Entries'].values\n",
    "\n",
    "    # Create and fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict Real_Total_Entries using the linear regression model\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Plot the linear regression line on the entire dataset\n",
    "    plt.plot(subset['Year'], predictions, label=f'Linear Regression - {combination[\"Item_Description\"]} - {combination[\"District\"]}')\n",
    "\n",
    "    plt.scatter(subset['Year'], subset['Real_Total_Entries'], label='Actual Real_Total_Entries', alpha=0.5)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Real_Total_Entries')\n",
    "    plt.legend()\n",
    "    plt.title(f'Linear Regression for Sum of Real_Total_Entries over Time - {combination[\"Item_Description\"]} - {combination[\"District\"]}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90420d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Filter data for the years 2010-2023\n",
    "df_filtered = df[(df['Year'] >= 2006) & (df['Year'] <= 2023)]\n",
    "\n",
    "# Group by unique combinations and sum 'Real_Total_Entries'\n",
    "grouped_data = df_filtered.groupby(['Item_Description', 'District', 'Year'])['Real_Total_Entries'].sum().reset_index()\n",
    "\n",
    "# Leave-one-out cross-validation for Linear Regression\n",
    "for index, combination in grouped_data.iterrows():\n",
    "    plt.figure(figsize=(12, 8))  # Create a new figure for each unique combination\n",
    "\n",
    "    subset = grouped_data[\n",
    "        (grouped_data['Item_Description'] == combination['Item_Description']) &\n",
    "        (grouped_data['District'] == combination['District'])\n",
    "    ]\n",
    "\n",
    "    # Initialize arrays to store predictions and actual values\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    # Iterate through each data point\n",
    "    for i, (_, row) in enumerate(subset.iterrows()):\n",
    "        X_train = subset.loc[subset.index != i, 'Year'].values.reshape(-1, 1)\n",
    "        y_train = subset.loc[subset.index != i, 'Real_Total_Entries'].values\n",
    "\n",
    "        X_test = np.array(row['Year']).reshape(-1, 1)\n",
    "        y_test = np.array(row['Real_Total_Entries'])\n",
    "\n",
    "        # Create and fit the linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict Real_Total_Entries using the linear regression model\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "        # Store predictions and actual values\n",
    "        predictions.append(prediction[0])\n",
    "        actual_values.append(y_test)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(actual_values, predictions)\n",
    "    print(f\"MAE for {combination['Item_Description']} - {combination['District']}: {mae}\")\n",
    "\n",
    "    # Plot the overall linear regression line for each unique combination\n",
    "    plt.plot(subset['Year'], predictions, label=f'Linear Regression - {combination[\"Item_Description\"]} - {combination[\"District\"]}', linestyle='dashed', color='red')\n",
    "\n",
    "    # Plot the actual data points\n",
    "    plt.scatter(subset['Year'], actual_values, label='Actual Real_Total_Entries', alpha=0.5)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Real_Total_Entries')\n",
    "    plt.legend()\n",
    "    plt.title(f'Linear Regression for Sum of Real_Total_Entries over Time - {combination[\"Item_Description\"]} - {combination[\"District\"]}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c4ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34533ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Filter data for the years 2010-2023\n",
    "df_filtered = df[(df['Year'] >= 2010) & (df['Year'] <= 2023)]\n",
    "\n",
    "# Group by unique combinations and sum 'Real_Total_Entries'\n",
    "grouped_data = df_filtered.groupby(['Item_Description', 'District', 'Year'])['Real_Total_Entries'].sum().reset_index()\n",
    "\n",
    "# Leave-one-out cross-validation for Weighted Linear Regression\n",
    "for (item, district), subset in grouped_data.groupby(['Item_Description', 'District']):\n",
    "    plt.figure(figsize=(12, 8))  # Create a new figure for each unique combination\n",
    "\n",
    "    # Initialize arrays to store predictions and actual values\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    # Iterate through each data point\n",
    "    for i, (_, row) in enumerate(subset.iterrows()):\n",
    "        X_train = subset.loc[subset.index != i, 'Year'].values.reshape(-1, 1)\n",
    "        y_train = subset.loc[subset.index != i, 'Real_Total_Entries'].values\n",
    "\n",
    "        X_test = np.array(row['Year']).reshape(-1, 1)\n",
    "        y_test = np.array(row['Real_Total_Entries'])\n",
    "\n",
    "        # Create and fit the weighted linear regression model\n",
    "        sample_weights = np.linspace(0.0001, 1, len(X_train))\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "        # Predict Real_Total_Entries using the weighted linear regression model\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "        # Store predictions and actual values\n",
    "        predictions.append(prediction[0])\n",
    "        actual_values.append(y_test)\n",
    "\n",
    "    # Calculate RMSE for the year 2023\n",
    "    rmse_2023 = np.sqrt(mean_squared_error(actual_values[-1:], predictions[-1:]))\n",
    "    print(f\"RMSE for {item} - {district} in 2023: {rmse_2023}\")\n",
    "\n",
    "    # Calculate MAPE for the year 2023\n",
    "    mape_2023 = np.mean(np.abs((actual_values[-1] - predictions[-1]) / actual_values[-1])) * 100\n",
    "    print(f\"MAPE for {item} - {district} in 2023: {mape_2023}\")\n",
    "\n",
    "    # Plot the overall weighted linear regression line for each unique combination\n",
    "    plt.plot(subset['Year'], predictions, label=f'Weighted Linear Regression - {item} - {district}', linestyle='dashed', color='red')\n",
    "\n",
    "    # Plot the actual data points\n",
    "    plt.scatter(subset['Year'], actual_values, label='Actual Real_Total_Entries', alpha=0.5)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Real_Total_Entries')\n",
    "    plt.legend()\n",
    "    plt.title(f'Weighted Linear Regression for Sum of Real_Total_Entries over Time - {item} - {district}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert 'Year' to datetime type\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Filter data for the years 2010-2023\n",
    "df_filtered = df[(df['Year'] >= '2006') & (df['Year'] <= '2023')]\n",
    "\n",
    "# Group by 'Item_Description', 'District', and 'Year' and calculate the sum of 'Real_Total_Entries'\n",
    "df_combined = df_filtered.groupby(['Item_Description', 'District', 'Year'])['Real_Total_Entries'].sum().reset_index()\n",
    "\n",
    "# Save the DataFrame to a new Excel file\n",
    "output_file_path =  r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx'\n",
    "df_combined.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arima Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Load data from Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx')\n",
    "\n",
    "# Convert 'Year' column to datetime\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Function to calculate accuracy metrics\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure there are enough data points for modeling\n",
    "if len(df) > 3:  # Updated condition to ensure at least four data points\n",
    "\n",
    "    # Group data by 'item description' and 'district'\n",
    "    grouped_data = df.groupby(['Item_Description', 'District'])\n",
    "\n",
    "    # Iterate through each group\n",
    "    for (item, district), group_df in grouped_data:\n",
    "\n",
    "        # Check if there is data for the year 2023\n",
    "        if 2023 in group_df['Year'].dt.year.values:\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            train_size = int(len(group_df) * 0.8)  # 80% for training, 20% for testing\n",
    "            train, test = group_df.iloc[:train_size], group_df.iloc[train_size:]\n",
    "\n",
    "            # Check if the length of the training data is sufficient for the ARIMA order\n",
    "            if len(train) > 2 + 2 + 2 + 1:  # Adjust the order as needed\n",
    "                # Fit the ARIMA model to the training data\n",
    "                model = ARIMA(train['Real_Total_Entries'], order=(2, 2, 4))  # Adjust the order as needed\n",
    "                results = model.fit()\n",
    "\n",
    "                # Make predictions for the test data\n",
    "                predicted_values = results.predict(start=len(train), end=len(group_df)-1, typ='levels')\n",
    "\n",
    "                # Plotting actual vs predicted values for the test set\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(test['Year'], test['Real_Total_Entries'], label='Actual')\n",
    "                plt.plot(test['Year'], predicted_values, label='Predicted', linestyle='dashed', color='red')\n",
    "                plt.title(f\"ARIMA Model - {item}, {district}\")\n",
    "                plt.xlabel('Year')\n",
    "                plt.ylabel('Real_Total_Entries')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # Calculate RMSE for the test set\n",
    "                true_values = test['Real_Total_Entries']\n",
    "                rmse = calculate_rmse(true_values, predicted_values)\n",
    "\n",
    "                # Print RMSE\n",
    "                print(f\"RMSE for {item}, {district}: {rmse}\")\n",
    "            else:\n",
    "                print(f\"Not enough data for modeling ARIMA for {item}, {district}.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data for modeling.\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbae7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Load data from Excel file\n",
    "# Replace 'your_data.xlsx' with the actual path to your Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx')\n",
    "\n",
    "# Convert 'Year' column to datetime\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Function to calculate accuracy metrics\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure there are enough data points for modeling\n",
    "if len(df) > 3:  # Updated condition to ensure at least four data points\n",
    "\n",
    "    # Group data by 'item description' and 'district'\n",
    "    grouped_data = df.groupby(['Item_Description', 'District'])\n",
    "\n",
    "    # Iterate through each group\n",
    "    for (item, district), group_df in grouped_data:\n",
    "\n",
    "        # Check if there is data for the year 2023\n",
    "        if 2023 in group_df['Year'].dt.year.values:\n",
    "\n",
    "            # Fit the exponential smoothing model to the group\n",
    "            model = ExponentialSmoothing(group_df['Real_Total_Entries'], trend='add', seasonal=None)\n",
    "\n",
    "            # Adjust the smoothing_slope parameter to give more weight to the most recent values\n",
    "            results = model.fit(smoothing_slope=0.4)  # You can experiment with different values\n",
    "\n",
    "            # Make predictions for every year\n",
    "            predicted_values = results.predict(start=0, end=len(group_df)-1)\n",
    "\n",
    "            # Plotting actual vs predicted values\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(group_df['Year'], group_df['Real_Total_Entries'], label='Actual')\n",
    "            plt.plot(group_df['Year'], predicted_values, label='Predicted', linestyle='dashed', color='red')\n",
    "            plt.title(f\"Exponential Smoothing Model - {item}, {district}\")\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel('Real_Total_Entries')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate RMSE for all years\n",
    "            true_values = group_df['Real_Total_Entries']\n",
    "            rmse = calculate_rmse(true_values, predicted_values)\n",
    "\n",
    "            # Print RMSE\n",
    "            print(f\"RMSE for {item}, {district}: {rmse}\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data for modeling.\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from Excel file\n",
    "# Replace 'your_data.xlsx' with the actual path to your Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx')\n",
    "\n",
    "# Convert 'Year' column to datetime\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Function to calculate accuracy metrics\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure there are enough data points for modeling\n",
    "if len(df) > 3:  # Updated condition to ensure at least four data points\n",
    "\n",
    "    # Group data by 'item description' and 'district'\n",
    "    grouped_data = df.groupby(['Item_Description', 'District'])\n",
    "\n",
    "    # Iterate through each group\n",
    "    for (item, district), group_df in grouped_data:\n",
    "\n",
    "        # Check if there is data for the year 2023 and if there are enough data points for training\n",
    "        if 2023 in group_df['Year'].dt.year.values and len(group_df) > 1:\n",
    "\n",
    "            # Split data into training and testing sets\n",
    "            train_size = int(len(group_df) * 0.8)\n",
    "            train, test = group_df[:train_size], group_df[train_size:]\n",
    "\n",
    "            # Check if the training set has enough data points\n",
    "            if len(train) > 1:\n",
    "                # Fit the exponential smoothing model to the training data\n",
    "                model = ExponentialSmoothing(train['Real_Total_Entries'], trend='add', seasonal=None)\n",
    "                results = model.fit(smoothing_slope=0.7)  # You can experiment with different values\n",
    "\n",
    "                # Make predictions for the test data\n",
    "                predicted_values = results.predict(start=len(train), end=len(group_df)-1)\n",
    "\n",
    "                # Plotting actual vs predicted values\n",
    "                # Plotting actual vs predicted values\n",
    "                # Plotting actual vs predicted values\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(train['Year'], train['Real_Total_Entries'], label='Actual - Training Set')\n",
    "                plt.plot(test['Year'], test['Real_Total_Entries'], label='Actual - Test Set')\n",
    "                plt.plot(test['Year'], predicted_values, label='Predicted', linestyle='dashed', color='red')\n",
    "                plt.title(f\"Exponential Smoothing Model - {item}, {district}\")\n",
    "                plt.xlabel('Year')\n",
    "                plt.ylabel('Real_Total_Entries')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                # Calculate RMSE for the test set\n",
    "                true_values = test['Real_Total_Entries']\n",
    "                rmse = calculate_rmse(true_values, predicted_values)\n",
    "\n",
    "                # Print RMSE\n",
    "                print(f\"RMSE for {item}, {district}: {rmse}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Not enough data in the training set for {item}, {district}.\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data for modeling.\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc84491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression model MAPE\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Final Calculation Folder\\filled_data - Copy.xlsx'  # Make sure to update the path to your actual file location\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Filter data for the years 2006-2023\n",
    "df_filtered = df[(df['Year'] >= 2006) & (df['Year'] <= 2023)]\n",
    "\n",
    "# Group by unique combinations and sum 'Real_Total_Entries'\n",
    "grouped_data = df_filtered.groupby(['Item_Description', 'District', 'Year'])['Real_Total_Entries'].sum().reset_index()\n",
    "\n",
    "# Initialize a list to store MAPE values for each unique combination\n",
    "mape_values = []\n",
    "\n",
    "# Leave-one-out cross-validation for Linear Regression\n",
    "for index, combination in grouped_data.iterrows():\n",
    "    subset = grouped_data[\n",
    "        (grouped_data['Item_Description'] == combination['Item_Description']) &\n",
    "        (grouped_data['District'] == combination['District'])\n",
    "    ]\n",
    "\n",
    "    # Initialize arrays to store predictions and actual values\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    # Iterate through each data point\n",
    "    for i, (_, row) in enumerate(subset.iterrows()):\n",
    "        X_train = subset.loc[subset.index != i, 'Year'].values.reshape(-1, 1)\n",
    "        y_train = subset.loc[subset.index != i, 'Real_Total_Entries'].values\n",
    "\n",
    "        X_test = np.array(row['Year']).reshape(-1, 1)\n",
    "        y_test = np.array(row['Real_Total_Entries'])\n",
    "\n",
    "        # Create and fit the linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict Real_Total_Entries using the linear regression model\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "        # Store predictions and actual values\n",
    "        predictions.append(prediction[0])\n",
    "        actual_values.append(y_test)\n",
    "\n",
    "    # Calculate MAPE for the current combination\n",
    "    mape = np.mean(np.abs((np.array(actual_values) - np.array(predictions)) / np.array(actual_values))) * 100\n",
    "    mape_values.append(mape)\n",
    "\n",
    "# Calculate the total average MAPE across all unique combinations\n",
    "total_average_mape = np.mean(mape_values)\n",
    "print(f\"Total Average MAPE: {total_average_mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3352240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arima MAPE\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Load data from Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx')\n",
    "\n",
    "# Convert 'Year' column to datetime\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_index = y_true != 0  # Avoid division by zero\n",
    "    if np.any(non_zero_index):\n",
    "        return np.mean(np.abs((y_true[non_zero_index] - y_pred[non_zero_index]) / y_true[non_zero_index])) * 100\n",
    "    else:\n",
    "        return np.nan  # Return NaN if all y_true values are zero to avoid division by zero\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize a list to store MAPE values for each group\n",
    "mape_values = []\n",
    "\n",
    "# Ensure there are enough data points for modeling\n",
    "if len(df) > 3:\n",
    "    grouped_data = df.groupby(['Item_Description', 'District'])\n",
    "\n",
    "    for (item, district), group_df in grouped_data:\n",
    "        if 2023 in group_df['Year'].dt.year.values:\n",
    "            train_size = int(len(group_df) * 0.8)\n",
    "            train, test = group_df.iloc[:train_size], group_df.iloc[train_size:]\n",
    "\n",
    "            if len(train) > 3:  # Ensure there's enough data\n",
    "                model = ARIMA(train['Real_Total_Entries'], order=(2, 2, 4))\n",
    "                results = model.fit()\n",
    "\n",
    "                predicted_values = results.predict(start=len(train), end=len(group_df)-1, typ='levels')\n",
    "\n",
    "                # No plotting part for calculating MAPE\n",
    "                \n",
    "                true_values = test['Real_Total_Entries']\n",
    "                mape = calculate_mape(true_values, predicted_values)\n",
    "                \n",
    "                # Storing MAPE value for each combination\n",
    "                mape_values.append(mape)\n",
    "                print(f\"MAPE for {item}, {district}: {mape}%\")\n",
    "            else:\n",
    "                print(f\"Not enough data for modeling ARIMA for {item}, {district}.\")\n",
    "else:\n",
    "    print(\"Not enough data for modeling.\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Calculate the total average MAPE across all groups\n",
    "if mape_values:\n",
    "    total_average_mape = np.mean(mape_values)\n",
    "    print(f\"Total Average MAPE: {total_average_mape}%\")\n",
    "else:\n",
    "    print(\"No MAPE values calculated, possibly due to insufficient data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential Smooothing Mape\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Load data from Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\Sam Beighle\\Documents\\Capstone - FWP Draw\\Testing\\Test_Data_ElkDeer_Sum.xlsx')\n",
    "\n",
    "# Convert 'Year' column to datetime\n",
    "df['Year'] = pd.to_datetime(df['Year'], format='%Y')\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    non_zero_index = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_index] - y_pred[non_zero_index]) / y_true[non_zero_index])) * 100\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mape_values = []  # Store MAPE values for each group\n",
    "\n",
    "# Ensure there are enough data points for modeling\n",
    "if len(df) > 3:\n",
    "    grouped_data = df.groupby(['Item_Description', 'District'])\n",
    "\n",
    "    for (item, district), group_df in grouped_data:\n",
    "        if 2023 in group_df['Year'].dt.year.values:\n",
    "            model = ExponentialSmoothing(group_df['Real_Total_Entries'], trend='add', seasonal=None)\n",
    "            results = model.fit(smoothing_slope=0.4)  # You can experiment with different values\n",
    "\n",
    "            predicted_values = results.predict(start=0, end=len(group_df)-1)\n",
    "            true_values = group_df['Real_Total_Entries']\n",
    "\n",
    "            # Calculate MAPE for each group\n",
    "            mape = calculate_mape(true_values, predicted_values)\n",
    "            mape_values.append(mape)\n",
    "            print(f\"MAPE for {item}, {district}: {mape}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data for modeling.\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Calculate the total average MAPE across all groups\n",
    "if mape_values:\n",
    "    total_average_mape = np.mean(mape_values)\n",
    "    print(f\"Total Average MAPE: {total_average_mape}%\")\n",
    "else:\n",
    "    print(\"No MAPE values calculated, possibly due to insufficient data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc334494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea678f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a32f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208288d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
